{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4077a8c1-5a5c-4745-9f3b-36a336d0f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from generalist.generalist_datasets.coco.coco import (\n",
    "    CocoCaption,\n",
    "    CocoCaptionTargetTranform,\n",
    "    CocoFilepaths,\n",
    "    CocoImageTransforms,\n",
    ")\n",
    "from generalist.eval import preliminary_eval\n",
    "from generalist.generalist_datasets.hf.summary import BillSum, XSum, SummaryTransforms\n",
    "from generalist.generalist_datasets.utils.data_collate import collate_func_helper\n",
    "from generalist.generalist_datasets.utils.multiple_datasets import ChainedDataset\n",
    "from generalist.generalist_tokenizers import image_tokenizers, text_tokenizers\n",
    "from generalist.models.embedding_model import EmbeddingModel\n",
    "from generalist.models.model import GeneralistModel\n",
    "from generalist.models.output_model import GeneralOutput\n",
    "from generalist.predict import ImageCaptionPrediction\n",
    "from generalist.utils.display.display import GeneralistDisplay\n",
    "from generalist.utils.utils import get_hostname, save_checkpoint\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af08e0b-9a17-4cf9-98ae-a33b7dd343ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = compose(config_name=get_hostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbaa2e43-db0a-4dde-a997-9b0bd7997f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'TextTokenizerBert'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/data/graham/datasets/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
     ]
    }
   ],
   "source": [
    "model_save_dir = Path(cfg.model_save_dir)\n",
    "display_flag = cfg.display.display_flag\n",
    "device = cfg.device\n",
    "context_length = cfg.context_length\n",
    "\n",
    "learning_rate = cfg.training.learning_rate\n",
    "batch_size = cfg.training.batch_size\n",
    "n_epochs = cfg.training.n_epochs\n",
    "\n",
    "model_dim = cfg.model.model_dim\n",
    "\n",
    "image_tokenizer = image_tokenizers.ImageTokenizer(device=device)\n",
    "text_tokenizer = text_tokenizers.TextTokenizerBert.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "embedding_model = EmbeddingModel(model_dim=model_dim)\n",
    "# output_model = GeneralClassificationOutput(model_dim=model_dim, num_classes=10, reduce_type=\"cls\")\n",
    "output_model = GeneralOutput(model_dim=model_dim, output_dim=text_tokenizer.vocab_size)\n",
    "model = GeneralistModel(output_model=output_model, **cfg.model).to(device)\n",
    "\n",
    "start_tokens = torch.Tensor([text_tokenizer.cls_token_id]).to(device).to(int)\n",
    "\n",
    "embedding_model.to(device)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": embedding_model.parameters()},\n",
    "        {\"params\": model.parameters()},\n",
    "    ],\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "tokenizers = [image_tokenizer, text_tokenizer]\n",
    "\n",
    "text_tokenizer_kwargs = cfg.text_tokenizer\n",
    "\n",
    "coco_filepaths = CocoFilepaths(base_dir=cfg.coco_dir, split=\"train\")\n",
    "\n",
    "coco_caption = CocoCaption(\n",
    "    root=coco_filepaths.images_root,\n",
    "    annFile=coco_filepaths.captions_filepath,\n",
    "    transform=CocoImageTransforms.train,\n",
    "    target_transform=CocoCaptionTargetTranform.get(text_tokenizer=text_tokenizer, text_tokenizer_kwargs=text_tokenizer_kwargs).train,\n",
    ")\n",
    "\n",
    "summary_dataset = XSum(\n",
    "    text_transform=SummaryTransforms.make_transforms(text_tokenizer=text_tokenizer, text_tokenizer_kwargs=text_tokenizer_kwargs).train,\n",
    ")\n",
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e46453c-e506-4fb5-8f52-7c95f6449e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<evaluate.evaluator.text2text_generation.SummarizationEvaluator at 0x7f0c00a45840>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57668283-70da-4e75-8b05-0f92e6aa9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline:\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        self.embedding_model = embedding_model\n",
    "    def __call__(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0ae704-1261-4d81-a674-a6df3a9a91c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = summary_dataset[0]\n",
    "embedded_data = embedding_model(sample.data.to(device))\n",
    "embedded_tgt = embedding_model(sample.target.to(device))\n",
    "\n",
    "tgt_mask = model.get_tgt_mask_tri(embedded_tgt=embedded_tgt)\n",
    "logits = model(embedded_data, embedded_tgt=embedded_tgt, tgt_key_padding_mask=None, tgt_mask=tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3076622-7faa-4ee0-a7b7-74f4113fdfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0952,  0.4816,  0.5421,  ...,  0.0427, -1.1204, -0.8917],\n",
       "         [ 0.2325,  0.5230,  0.6460,  ...,  0.4650, -0.9121, -0.9191],\n",
       "         [ 0.4001,  0.7019,  0.7704,  ...,  0.6375, -1.5120, -0.7964],\n",
       "         ...,\n",
       "         [ 0.7627,  0.5170,  0.5903,  ...,  0.2748, -1.2592, -0.8494],\n",
       "         [ 0.6674,  0.7275,  0.8456,  ...,  0.3820, -1.1086, -1.1657],\n",
       "         [ 0.8795,  0.6612,  0.7759,  ...,  0.6136, -1.0662, -0.9765]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d901655-aba5-4dc3-a2ca-8e1c9ca92f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate.evaluator.text2text_generation.SummarizationEvaluator"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_evaluator.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcbc0f6c-a80e-46a6-8cc9-e3a009473e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] the full cost of damage in newton stewart, one of the areas worst affected, is still being assessed. repair work is ongoing in hawick and many roads in peeblesshire remain badly affected by standing water. trains on the west coast mainline face disruption due to damage at the lamington viaduct. many businesses and householders were affected by flooding in newton stewart after the river cree overflowed into the town. first minister nicola sturgeon visited the area to inspect the damage. the waters breached a retaining wall, flooding many commercial properties on victoria street - the main shopping thoroughfare. jeanette tate, who owns the cinnamon cafe which was badly affected, said she could not fault the multi - agency response once the flood hit. however, she said more preventative work could have been carried out to ensure the retaining wall did not fail. \" it is difficult but i do think there is so much publicity for dumfries and the nith - and i totally appreciate that - but it is almost like we\\'re neglected or forgotten, \" she said. \" that may not be true but it is perhaps my perspective over the last few days. \" why were you not ready to help us a bit more when the warning and the alarm alerts had gone out? \" meanwhile, a flood alert remains in place across the borders because of the constant rain. peebles was badly hit by problems, sparking calls to introduce more defences in the area. scottish borders council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs. the labour party\\'s deputy scottish leader alex rowley was in hawick on monday to see the situation first hand. he said it was important to get the flood protection plan right but backed calls to speed up the process. \" i was quite taken aback by the amount of damage that has been done, \" he said. \" obviously it is heart - breaking for people who have been forced out of their [SEP]']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenizer.batch_decode(sample.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3722fa77-b9d0-4ecc-98db-6450073accc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|████████████████████████████████████████████████████████████████████████████████████████| 8.41k/8.41k [00:00<00:00, 3.77MB/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9df6c8a-f27b-425a-a908-5650b46972ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96cee2d2-861e-4c23-9efb-429201e38039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from rouge_score) (1.2.0)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from rouge_score) (1.23.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from nltk->rouge_score) (2022.9.13)\n",
      "Requirement already satisfied: click in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/graham/mambaforge/envs/p310/lib/python3.10/site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=9889440783bb88370444dd9a1caefb14919c8c7144da2f81ffbf918dee637217\n",
      "  Stored in directory: /home/graham/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: joblib, nltk, rouge_score\n",
      "Successfully installed joblib-1.2.0 nltk-3.7 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b092908a-06a2-4bab-b034-a8ee22257e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b75dc30-2b35-4496-839c-6ab8580e56c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23606d7c-c83b-4690-8d04-a5d8b2cfbba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: 1,\nInput references: 13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m rouge_score\u001b[38;5;241m.\u001b[39mcompute(\n\u001b[1;32m      2\u001b[0m     predictions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m], references\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m scores\n",
      "File \u001b[0;32m~/mambaforge/envs/p310/lib/python3.10/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/p310/lib/python3.10/site-packages/evaluate/module.py:480\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/p310/lib/python3.10/site-packages/evaluate/module.py:552\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, v[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/p310/lib/python3.10/site-packages/evaluate/module.py:572\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m    565\u001b[0m feature_strings \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature option \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)])\n\u001b[1;32m    566\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfeature_strings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreferences\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format:\nFeature option 0: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}\nFeature option 1: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')},\nInput predictions: 1,\nInput references: 13"
     ]
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[1, 3, 5, 10], references=[13, 5, 10]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef22e8-c425-45fe-89de-da5721516ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
