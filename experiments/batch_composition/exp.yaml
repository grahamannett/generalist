wandb:
  project: genearlist_batch_composition
  entity: graham
  tags: null

stats_file: null

datasets:
  train:
    xsum:
      task: summary
      split: train
    cococaption:
      name: cococaption
      task: image caption
      split: train
  test:
    xsum:
      task: summary
      split: test
    cococaption:
      name: cococaption
      task: image caption
      split: val


model:
  combine_embeddings: False
  model_dim: 768
  # latent_seq_len: 32
  encoder_nhead: 3 # was 4
  decoder_nhead: 3 # was 4
  encoder_num_layers: 3 # was 4
  decoder_num_layers: 3 # was 4
  enable_nested_tensor: True
  use_encoder: True

training:
  batch_size: 16
  learning_rate: 5e-5
  n_epochs: 10
  batch_uniform_dataset_samples: False
  end_early: null


predictions:
  initial_generation: True

text_tokenizer:
  name: bert-base-uncased
  vocab_size: 30522 # probably comes from pretrained text_tokenizer
  encode_kwargs:
    return_tensors: pt
    truncation: True
    padding: max_length
    max_length: 400 #  was 512, temporary workaround so text and image have same dim
    return_attention_mask: True

